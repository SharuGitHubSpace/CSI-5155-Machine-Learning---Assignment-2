{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2df6dcf",
   "metadata": {},
   "source": [
    "\n",
    "# Saranya 300321456 - Assignment 2 - Evaluation of Learning - MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce77b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as bsns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from imblearn.pipeline import Pipeline, make_pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_selection import SelectKBest , f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26fc0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import libraries\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    " \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7155d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\n",
    "def category_to_number(X, to_one_hot = [], to_ordinal =[]):\n",
    "    '''\n",
    "    Input:  X - original dataset\n",
    "            to_one_hot - the columns that need to be One hot encoded\n",
    "            to_ordinal  - the columns that need to be oridinal encoded\n",
    "    \n",
    "    Output: X - after all the processing done\n",
    "    \n",
    "    Function to change the categorical variables present in the dataset to encoded numericals   \n",
    "    \n",
    "    '''\n",
    "    OHen = OneHotEncoder()\n",
    "    Oren = OrdinalEncoder()\n",
    "    n = X.shape[1]\n",
    "    # looping through the columns and encodering the categorical column     \n",
    "    for i in range(n):\n",
    "        \n",
    "        # One hot encoding for cardinal columns\n",
    "        if not to_one_hot:\n",
    "            if X.iloc[:,i].name in to_one_hot:\n",
    "                OHE = OHen.fit_transform(X.iloc[:,[i]])\n",
    "                df = pd.DataFrame(OHE.toarray(),columns=[X.iloc[:,i].name+\"_\"+y for y in X.iloc[:,i].unique()])\n",
    "                X = pd.merge(left=X,right=df,left_index=True,right_index=True)\n",
    "        \n",
    "        # Ordinal encoding for ordinal columns\n",
    "        if X.iloc[:,i].name in to_ordinal:\n",
    "            X.loc[:,X.iloc[:,i].name] = Oren.fit_transform(X.iloc[:,[i]])\n",
    "    \n",
    "    # Dropping the cardinal categorical columns to avoind redundancy  \n",
    "    X.drop(columns=to_one_hot,inplace = True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da832a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_selection(X,y):\n",
    "    noOfFeaturesSelected = 8\n",
    "    from sklearn.feature_selection import SelectKBest , f_classif\n",
    "    selector = SelectKBest(f_classif, k=noOfFeaturesSelected)\n",
    "    selector.fit(X,y) \n",
    "    selector.scores_\n",
    "\n",
    "    # Significant Features for Alcohol\n",
    "    cols = selector.get_support(indices=True)    \n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25c12143",
   "metadata": {},
   "outputs": [],
   "source": [
    " # define the model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "models = []\n",
    "#models.append(('SVM',SVC__params,SVC(kernel='linear',random_state=4)))\n",
    "#models.append(('DT',DT__params,DecisionTreeClassifier()))\n",
    "#models.append(('RF',RF__params,RandomForestClassifier(n_estimators=40)))\n",
    "#models.append(('KNN',KNN__Params,KNeighborsClassifier(n_neighbors=4)))\n",
    "models.append(('MLP',MLP__params,MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(256,128,64,32), activation=\"relu\", random_state=1)))\n",
    "#models.append(('GB',GB__params,GradientBoostingClassifier(random_state=42)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69568a29",
   "metadata": {},
   "source": [
    "# VSA - Dataset with Highest prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58e11b",
   "metadata": {},
   "source": [
    "1) Load Data set\n",
    "2) Feature Scaling\n",
    "3) Feature Selecton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa6f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "def get_dataset():\n",
    "    # DF -- VSA\n",
    "    path = 'C:/Users/VaishuSistas/Desktop/ML/Assignments/Assignment1/Data/Assignment1_drug_consumption.txt'\n",
    "    dfmainVSA  = pd.read_csv(path,usecols=[1,2,3,4,5,6,7,8,9,10,11,12,31]) \n",
    "    dfmainVSA.columns = ['Age','Gender','Education','Country','Ethnicity',\n",
    "                         'NScore','Escore','OScore','AScore','CScore',\n",
    "                         'Impulse','SS','DrugClass'] \n",
    "    dfmainVSA['DrugClass'] = dfmainVSA ['DrugClass'].apply(lambda x: '0' if (x == 'CL1' or x == 'CL0') else '1')\n",
    "    dfmainVSA.head(20)\n",
    "    dfmainVSA.groupby('DrugClass').mean()\n",
    "\n",
    "    X = dfmainVSA[dfmainVSA.columns[~dfmainVSA.columns.isin(['DrugClass'])]]\n",
    "    y = dfmainVSA['DrugClass']\n",
    "    dfmainVSA.groupby(['DrugClass'])['DrugClass'].count()\n",
    "    # Feature Scaling - Drug DS\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X)) \n",
    "    \n",
    "    # Feature selection\n",
    "    cols = Feature_selection(X,y)\n",
    "    X = X.iloc[:,cols]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a5210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Labour negotiations dataset\n",
    "def Get_Labordataset():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    labour_data_1 = pd.read_csv(\"LabourDataset/labor-neg.data\",na_values=\"?\")\n",
    "    labour_data_2 =pd.read_csv(\"LabourDataset/labor-neg.test\",na_values=\"?\")\n",
    "    labour_data = labour_data_1.append(labour_data_2, ignore_index = True)\n",
    "\n",
    "    # Features split and output class target variable\n",
    "    labour_X = labour_data.iloc[:,:-1]\n",
    "    labour_y = labour_data.iloc[:,-1]\n",
    "\n",
    "    # Checking missing values in dataset\n",
    "    #print((np.sum(labour_X.isnull(),axis=0)))\n",
    "\n",
    "    # Dropping columns with missing values more than 20\n",
    "    labour_X.dropna(axis=1,thresh=labour_X.shape[0] - 20,inplace=True)\n",
    "\n",
    "\n",
    "    # Imputing missing values in numerical columns\n",
    "    num_imputer=SimpleImputer(strategy=\"mean\")\n",
    "    labour_X.loc[:,labour_X.dtypes == \"float64\"] = num_imputer.fit_transform(labour_X.loc[:,labour_X.dtypes == \"float64\"])\n",
    "\n",
    "    # Imputing missing values in categorical columns\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    labour_X.loc[:,labour_X.dtypes == \"object\"] = cat_imputer.fit_transform(labour_X.loc[:,labour_X.dtypes == \"object\"])\n",
    "\n",
    "    # converting categorival variables to numerical variables\n",
    "    column_names=labour_X.loc[:,labour_X.dtypes == \"object\"].columns\n",
    "\n",
    "    labour_X = category_to_number(labour_X,to_ordinal=column_names)\n",
    "    \n",
    "    # Feature Scaling\n",
    "        \n",
    "    Scaler_1 = MinMaxScaler(feature_range=(0, 1)).fit(labour_X)\n",
    "    labour_X = pd.DataFrame(Scaler_1.transform(labour_X)) \n",
    "    \n",
    "    # Feature Selection\n",
    "     \n",
    "    cols = Feature_selection(labour_X,labour_y)\n",
    "    labour_X = labour_X.iloc[:,cols]\n",
    "        \n",
    "    return labour_X, labour_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e857fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Labour negotiations dataset\n",
    "def Get_HeartDataset():\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    # Importing Heart disease dataset\n",
    "    Heart_dataset = pd.read_csv(\"HeartDisease/heart_cleveland_upload.csv\",sep=\",\")\n",
    "    # Features split and output class target variable\n",
    "    heart_X = Heart_dataset.iloc[:,:-1]\n",
    "    heart_y = Heart_dataset.iloc[:,-1] \n",
    " \n",
    "    # Checking missing values in dataset\n",
    "    #print((np.sum(heart_X.isnull(),axis=0)))\n",
    "\n",
    "    # Dropping columns with missing values more than 20\n",
    "    heart_X.dropna(axis=1,thresh=heart_X.shape[0] - 20,inplace=True)\n",
    "\n",
    "    # Imputing missing values in numerical columns\n",
    "    num_imputer=SimpleImputer(strategy=\"mean\")\n",
    "    heart_X.loc[:,heart_X.dtypes == \"float64\"] = num_imputer.fit_transform(heart_X.loc[:,heart_X.dtypes == \"float64\"])\n",
    "\n",
    "    # Imputing missing values in categorical columns\n",
    "    #cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    #heart_X.loc[:,heart_X.dtypes == \"object\"] = cat_imputer.fit_transform(heart_X.loc[:,heart_X.dtypes == \"object\"])\n",
    "\n",
    "    # converting categorival variables to numerical variables\n",
    "    column_names=heart_X.loc[:,heart_X.dtypes == \"object\"].columns\n",
    "   \n",
    "    heart_X = category_to_number(heart_X,to_ordinal=column_names)    \n",
    "    \n",
    "    # Feature Scaling\n",
    "    \n",
    "    Scaler_2 = MinMaxScaler(feature_range=(0, 1)).fit(heart_X)\n",
    "    heart_X = pd.DataFrame(Scaler_2.transform(heart_X)) \n",
    "    \n",
    "    # Feature Selection\n",
    "     \n",
    "    cols = Feature_selection(heart_X,heart_y)\n",
    "    heart_X = heart_X.iloc[:,cols]\n",
    "    \n",
    "    return heart_X, heart_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2f87d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the cross-validation procedure With Over and under sampling confgured\n",
    "def Cross_Validation(X,y,ToDoOversample = 0, ToDoUnderSample = 0):\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "        # enumerate splits\n",
    "        outer_results = list()\n",
    "\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        CrossValResults = [] \n",
    "        for train_index, test_index in cv_outer.split(X):\n",
    "            # split data\n",
    "            X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            # configure the cross-validation procedure\n",
    "            cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "            # define search space\n",
    "            Dataset = []  \n",
    "            ModelNames = []\n",
    "            for name,params,model in models:        \n",
    "                # define search\n",
    "                search = GridSearchCV(model, params, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "                if ToDoOversample == 1:\n",
    "                    # SMOTE OVER SAMPLING\n",
    "                    sm = SMOTE()\n",
    "                    X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train, y_train)\n",
    "                    X_train = X_train_oversampled\n",
    "                    y_train = y_train_oversampled\n",
    "\n",
    "                if ToDoUnderSample == 1:                   \n",
    "                    # Random Under Sampling\n",
    "                    rus = RandomUnderSampler(sampling_strategy=\"not minority\")\n",
    "                    X_train_undersampled, y_train_undersampled = rus.fit_resample(X_train, y_train)\n",
    "                    X_train = X_train_undersampled\n",
    "                    y_train = y_train_undersampled\n",
    "\n",
    "                # execute search\n",
    "                result = search.fit(X_train, y_train)\n",
    "                # get the best performing model fit on the whole training set\n",
    "                best_model = result.best_estimator_\n",
    "                # evaluate model on the hold out dataset\n",
    "                Ypr = best_model.predict(X_test)\n",
    "                # evaluate the model\n",
    "                acc = accuracy_score(y_test, Ypr)\n",
    "                # store the result\n",
    "                outer_results.append(acc)\n",
    "                # report progress\n",
    "                # print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "                ModelNames.append(name)\n",
    "                CrossValResults.append(acc)\n",
    "                Dataset.append('Drug Dataset CV')\n",
    "        # summarize the estimated performance of the model\n",
    "        # print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "\n",
    "\n",
    "        for i in range(len(ModelNames)):\n",
    "            print(ModelNames[i],CrossValResults[i].mean() , Dataset[i])\n",
    "\n",
    "        return ModelNames,CrossValResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82bb6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "    DT__params = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [1, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              # \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "    SVC__params = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf','poly', 'sigmoid']} \n",
    "    \n",
    "    RF__params = { \n",
    "              'n_estimators': [200, 700],\n",
    "              'max_features': ['auto', 'sqrt', 'log2']\n",
    "                }\n",
    "    k_range = list(range(1, 31))\n",
    "    KNN__Params = dict(n_neighbors=k_range)\n",
    "    \n",
    "    MLP__params = {\n",
    "            'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'solver': ['sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.05],\n",
    "            'learning_rate': ['constant','adaptive'],\n",
    "                }\n",
    "    GB__params = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9727d37",
   "metadata": {},
   "source": [
    "### K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85604cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "kfold_validation=  KFold(n_splits=10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d47d28",
   "metadata": {},
   "source": [
    "### Hyper parameter Tuning Configurations Parameters for 6 Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "626de588",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y  = get_dataset()\n",
    "Labor_X,Labor_y = Get_Labordataset()\n",
    "Heart_X,Heart_y = Get_HeartDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f2348",
   "metadata": {},
   "source": [
    "### Drug data Set - Cross Validation - Experiment A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9c9027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.8941798941798942 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "DrugDS_CV_Scores = [] \n",
    "DrugDS_CV_Scores = Cross_Validation(X,y,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd00aa29",
   "metadata": {},
   "source": [
    "### Drug data Set - Over Sampling - Experiment B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0506930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.746031746031746 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "DrugDS_OvrSample_Scores = []\n",
    "DrugDS_OvrSample_Scores   = Cross_Validation(X,y,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21058731",
   "metadata": {},
   "source": [
    "### Drug data Set - Under Sampling - Experiment C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dc7ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.6402116402116402 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "DrugDS_UnderSample_Scores = []\n",
    "DrugDS_UnderSample_Scores = Cross_Validation(X,y,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2193da",
   "metadata": {},
   "source": [
    "#### Labor Relations Data set - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02cf1538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.8333333333333334 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "Labor_CV_Scores = [] \n",
    "Labor_CV_Scores = Cross_Validation(Labor_X,Labor_y,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221007f2",
   "metadata": {},
   "source": [
    "#### Labor Data set - Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b85a3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.8333333333333334 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "LaborDS_OvrSample_Scores = []\n",
    "LaborDS_OvrSample_Scores = Cross_Validation(Labor_X,Labor_y,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a02012",
   "metadata": {},
   "source": [
    "#### Labor Data set - Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d61433b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.8333333333333334 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "LaborDS_UnderSample_Scores = []\n",
    "LaborDS_UnderSample_Scores = Cross_Validation(Labor_X,Labor_y,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef198d",
   "metadata": {},
   "source": [
    "#### Heart Disease Data set - CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c99927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.7666666666666667 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "Heart_CV_Scores = []\n",
    "Heart_CV_Scores = Cross_Validation(Heart_X,Heart_y,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548050f",
   "metadata": {},
   "source": [
    "#### Heart Disease Data set - Over Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81320f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.7 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "Heart_OvrSample_Scores = []\n",
    "Heart_OvrSample_Scores = Cross_Validation(Heart_X,Heart_y,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362caf9",
   "metadata": {},
   "source": [
    "#### Heart Disease Data set - Under Sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf1dfd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 0.8 Drug Dataset CV\n"
     ]
    }
   ],
   "source": [
    "Heart_UnderSample_Scores = []\n",
    "Heart_UnderSample_Scores = Cross_Validation(Heart_X,Heart_y,0,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
